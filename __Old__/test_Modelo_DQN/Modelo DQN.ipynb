{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo a aplicar se basa en el artículo [Building a Recommendation System for Amazon Fashion Products using DQN (beta)](https://medium.com/@vibhu12345/building-a-recommendation-system-for-amazon-fashion-products-using-dqn-beta-855b1ff7834e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import deque\n",
    "from keras import layers, models\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(\"Netflix_Prize_data/netflix_data_sample.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de películas: 17675\n",
      "Cantidad de clientes: 18095\n"
     ]
    }
   ],
   "source": [
    "movie_count = data.movie_id.unique().shape[0]\n",
    "customer_count = data.customer_id.unique().shape[0]\n",
    "\n",
    "print(f\"Cantidad de películas: {movie_count}\")\n",
    "print(f\"Cantidad de clientes: {customer_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición del ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RecommendationEnv(gym.Env):\n",
    "    def __init__(self, states, states_dict, iterations = 10):\n",
    "        self.states = states\n",
    "        self.state = self.states[0]\n",
    "        self.states_dict = states_dict\n",
    "        self.iterations = iterations\n",
    "        self.index = 0\n",
    "        state.action = 0\n",
    "\n",
    "\n",
    "    def step(self, actions):\n",
    "        # Implement the transition logic based on the action\n",
    "        reward= 0\n",
    "        done= False\n",
    "        reviewerId= self.state.reviewerId\n",
    "        future_asins= [p for p in reviewers[reviewerId].products if self.states_dict[(p,reviewerId)].time>self.state.time]\n",
    "        matched_recommendations = False\n",
    "        #predicted recommendations\n",
    "        for i in actions:\n",
    "          if self.states[i].product_asin in future_asins:\n",
    "            self.action = i\n",
    "            matched_recommendations = True\n",
    "            break;\n",
    "\n",
    "\n",
    "        if matched_recommendations:\n",
    "            #Higher reward as they are bought products for the user in future\n",
    "            reward = 1\n",
    "        else:\n",
    "            self.action = actions[0]\n",
    "\n",
    "\n",
    "        self.index += 1\n",
    "        self.state = self.states[self.index]\n",
    "        print(f\"iteration :{self.index}\")\n",
    "        if (self.iterations == self.index): done = True\n",
    "\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def reset(self, iterations = 10):\n",
    "        # Reset the state to the initial position\n",
    "        self.state = self.states[0]\n",
    "        self.iterations = iterations\n",
    "        self.index = 0\n",
    "        return self.state\n",
    "\n",
    "# Create the custom environment\n",
    "env = RecommendationEnv(states_list, states, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición del Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 110\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[38;5;66;03m# and do the model fit!\u001b[39;00m\n\u001b[0;32m    107\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfit([np\u001b[38;5;241m.\u001b[39mtranspose(update_input_metadata),np\u001b[38;5;241m.\u001b[39mtranspose(update_input_ratings)], target, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[0;32m    108\u001b[0m                        epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 110\u001b[0m state_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43menv\u001b[49m\u001b[38;5;241m.\u001b[39mstates)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# Every other product can be a recommendation\u001b[39;00m\n\u001b[0;32m    112\u001b[0m action_size \u001b[38;5;241m=\u001b[39m state_size\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, states):\n",
    "        self.states = states\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Hiperparámetros para DQN\n",
    "        self.discount_factor = 0.99\n",
    "\n",
    "        # Tasa de aprendizaje para la red neuronal\n",
    "        self.learning_rate = 0.001\n",
    "        \n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.epsilon_min = 0.01\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "\n",
    "        # initialize target model\n",
    "        self.update_target_model()\n",
    "\n",
    "   \n",
    "    def build_model(self):\n",
    "\n",
    "        # Aproximar la función Q usando Red Neuronal\n",
    "        # entrada: estado actual (película actual que ve la persona)\n",
    "        # salida: Q-value para cada acción posible (cada película en el catálogo)\n",
    "\n",
    "\n",
    "        # Capa de entrada para el estado actual (película actual que ve la persona)\n",
    "        ### idea: agregar al input el rating actual\n",
    "        input_layer = tf.keras.layers.Input(shape=(1,), name='actual_state')\n",
    "\n",
    "        # Definición de capas ocultas\n",
    "        hidden_layer = layers.Dense(64, activation='relu')(input_layer)\n",
    "\n",
    "\n",
    "        # 1 Q-value por acción\n",
    "        output_layer = layers.Dense(100, activation='linear', name='q_values')(hidden_layer)\n",
    "\n",
    "        # Crear modelo\n",
    "        model = tf.keras.Model(inputs=input_layer, outputs=output_layer, name='DQN_model')\n",
    "\n",
    "        # Resumen del modelo\n",
    "        model.summary()\n",
    "\n",
    "        # Grafo de la red\n",
    "        # plot_model(model, to_file='Arquitectura_DQN.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        \n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "    def train_model(self):\n",
    "        # if len(self.memory) < self.train_start:\n",
    "        #     return\n",
    "        # batch_size = min(self.batch_size, len(self.memory))\n",
    "        # mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        # update_input_metadata =[]\n",
    "        # update_input_ratings =[]\n",
    "        # update_target_metadata = []\n",
    "        # update_target_ratings = []\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        # for i in range(self.batch_size):\n",
    "        #     update_input_metadata.append(np.array(mini_batch[i][0].metadata))\n",
    "        #     update_input_ratings.append(np.array(mini_batch[i][0].ratings))\n",
    "        #     action.append(mini_batch[i][1])\n",
    "        #     reward.append(mini_batch[i][2])\n",
    "        #     update_target_metadata.append(np.array(mini_batch[i][3].metadata))\n",
    "        #     update_target_ratings.append(np.array(mini_batch[i][3].ratings))\n",
    "        #     done.append(mini_batch[i][4])\n",
    "\n",
    "        target = self.model.predict([np.transpose(update_input_metadata),np.transpose(update_input_ratings)])\n",
    "        target_val = self.target_model.predict([np.transpose(update_target_metadata),np.transpose(update_target_ratings)])\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # Q Learning: get maximum Q value at s' from target model\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                target[i][action[i]] = reward[i] + self.discount_factor * ( np.amax(target_val[i]))\n",
    "\n",
    "        # and do the model fit!\n",
    "        self.model.fit([np.transpose(update_input_metadata),np.transpose(update_input_ratings)], target, batch_size=self.batch_size,\n",
    "                       epochs=1, verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # after some time interval update the target model to be same with model\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    # get recommendations from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.sample(range(self.action_size),10)\n",
    "        else:\n",
    "            q_value = self.model.predict([np.array([state.metadata]), np.array([state.ratings])])\n",
    "            return np.argpartition(q_value[0],-10)[-10:]\n",
    "\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "state_size = len(env.states)\n",
    "# Every other product can be a recommendation\n",
    "action_size = state_size\n",
    "agent = DQNAgent(state_size, action_size, env.states)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
